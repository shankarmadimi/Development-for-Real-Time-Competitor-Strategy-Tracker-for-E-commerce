{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install playwright nest_asyncio\n",
        "!playwright install chromium\n",
        "!apt-get install libatk1.0-0 libatk-bridge2.0-0 libatspi2.0-0 libxcomposite1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "iNO2hbUiUBsO",
        "outputId": "393b596e-a6a5-4540-c1d6-4c86dab4eafc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: playwright in /usr/local/lib/python3.12/dist-packages (1.57.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (1.6.0)\n",
            "Requirement already satisfied: pyee<14,>=13 in /usr/local/lib/python3.12/dist-packages (from playwright) (13.0.0)\n",
            "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /usr/local/lib/python3.12/dist-packages (from playwright) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from pyee<14,>=13->playwright) (4.15.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libatk-bridge2.0-0 is already the newest version (2.38.0-3).\n",
            "libatk1.0-0 is already the newest version (2.36.0-3build1).\n",
            "libatspi2.0-0 is already the newest version (2.44.0-3).\n",
            "libxcomposite1 is already the newest version (1:0.4.5-1build2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "import asyncio, json, csv, re\n",
        "from pathlib import Path\n",
        "from urllib.parse import urljoin\n",
        "from playwright.async_api import async_playwright\n",
        "\n",
        "BASE = \"https://books.toscrape.com/\"\n",
        "\n",
        "RATING_MAP = {\n",
        "    \"One\": 1,\n",
        "    \"Two\": 2,\n",
        "    \"Three\": 3,\n",
        "    \"Four\": 4,\n",
        "    \"Five\": 5\n",
        "}\n",
        "\n",
        "def parse_price(s):\n",
        "    if not s:\n",
        "        return None\n",
        "    m = re.search(r\"([0-9]+(?:\\.[0-9]+)?)\", s)\n",
        "    return float(m.group(1)) if m else None\n",
        "\n",
        "def parse_stock(s):\n",
        "    if not s:\n",
        "        return 0\n",
        "    m = re.search(r\"(\\d+)\", s)\n",
        "    return int(m.group(1)) if m else 0\n",
        "\n",
        "async def scrape_all_books():\n",
        "    rows = []\n",
        "    async with async_playwright() as p:\n",
        "        browser = await p.chromium.launch(headless=True)\n",
        "        ctx = await browser.new_context()\n",
        "        page = await ctx.new_page()\n",
        "\n",
        "        current_url = BASE\n",
        "        page_num = 1\n",
        "        book_counter = 0\n",
        "\n",
        "        print(\"ðŸ”¥ Starting scraperâ€¦ going to homepage...\\n\")\n",
        "        await page.goto(BASE, timeout=60000)\n",
        "\n",
        "        while True:\n",
        "            await page.wait_for_selector(\"article.product_pod\", timeout=30000)\n",
        "\n",
        "            print(f\"ðŸ“„ Scraping page {page_num} ...\")\n",
        "\n",
        "            cards = await page.query_selector_all(\"article.product_pod\")\n",
        "\n",
        "            for card in cards:\n",
        "                book_counter += 1\n",
        "\n",
        "                a = await card.query_selector(\"h3 > a\")\n",
        "                title = await a.get_attribute(\"title\")\n",
        "                rel_url = await a.get_attribute(\"href\")\n",
        "                product_url = urljoin(current_url, rel_url)\n",
        "\n",
        "                price_text = (await (await card.query_selector(\"p.price_color\")).text_content()).strip()\n",
        "                price = parse_price(price_text)\n",
        "                avail_text = (await (await card.query_selector(\"p.instock.availability\")).text_content()).strip()\n",
        "                stock = parse_stock(avail_text)\n",
        "                rating_class = await (await card.query_selector(\".star-rating\")).get_attribute(\"class\")\n",
        "\n",
        "                rating = 0\n",
        "                for k in RATING_MAP:\n",
        "                    if k in (rating_class or \"\"):\n",
        "                        rating = RATING_MAP[k]\n",
        "                        break\n",
        "\n",
        "                # Scrape details\n",
        "                detail = await ctx.new_page()\n",
        "                try:\n",
        "                    await detail.goto(product_url, timeout=60000)\n",
        "\n",
        "                    prod_table = await detail.query_selector_all(\"table.table.table-striped tr\")\n",
        "                    prod_info = {}\n",
        "                    for tr in prod_table:\n",
        "                        th = await tr.query_selector(\"th\")\n",
        "                        td = await tr.query_selector(\"td\")\n",
        "                        if th and td:\n",
        "                            key = (await th.text_content()).strip()\n",
        "                            val = (await td.text_content()).strip()\n",
        "                            prod_info[key] = val\n",
        "\n",
        "                    upc = prod_info.get(\"UPC\")\n",
        "\n",
        "                    desc_el = await detail.query_selector(\"#product_description + p\")\n",
        "                    description = (await desc_el.text_content()).strip() if desc_el else \"\"\n",
        "\n",
        "                    img = await detail.query_selector(\".carousel img\")\n",
        "                    img_src = await img.get_attribute(\"src\") if img else None\n",
        "                    image_url = urljoin(product_url, img_src) if img_src else None\n",
        "\n",
        "                except Exception:\n",
        "                    upc = None\n",
        "                    description = \"\"\n",
        "                    prod_info = {}\n",
        "                    image_url = None\n",
        "\n",
        "                finally:\n",
        "                    await detail.close()\n",
        "\n",
        "                rows.append({\n",
        "                    \"title\": title,\n",
        "                    \"product_url\": product_url,\n",
        "                    \"price\": price,\n",
        "                    \"price_text\": price_text,\n",
        "                    \"rating\": rating,\n",
        "                    \"stock\": stock,\n",
        "                    \"upc\": upc,\n",
        "                    \"description\": description,\n",
        "                    \"image_url\": image_url,\n",
        "                    **prod_info\n",
        "                })\n",
        "\n",
        "                # Print every 20 books for reassurance\n",
        "                if book_counter % 20 == 0:\n",
        "                    print(f\"   âž¤ Scraped {book_counter} books so farâ€¦ still going strong\")\n",
        "\n",
        "            print(f\"âœ… Completed page {page_num}\\n\")\n",
        "\n",
        "            # Next page\n",
        "            next_a = await page.query_selector(\"li.next > a\")\n",
        "            if not next_a:\n",
        "                break\n",
        "            next_href = await next_a.get_attribute(\"href\")\n",
        "            next_url = urljoin(current_url, next_href)\n",
        "            current_url = next_url\n",
        "            page_num += 1\n",
        "\n",
        "            await page.goto(next_url, timeout=60000)\n",
        "            await asyncio.sleep(0.4)\n",
        "\n",
        "        await browser.close()\n",
        "\n",
        "    print(f\"\\nðŸŽ‰ All done! Total books scraped = {book_counter}\")\n",
        "    return rows\n",
        "\n",
        "\n",
        "# Run the scraper\n",
        "data = asyncio.get_event_loop().run_until_complete(scrape_all_books())\n",
        "\n",
        "print(f\"\\nðŸ“¦ Final: collected {len(data)} books!\\n\")\n",
        "\n",
        "# Save output\n",
        "Path(\"output\").mkdir(exist_ok=True)\n",
        "csv_path = Path(\"output/books_all.csv\")\n",
        "json_path = Path(\"output/books_all.json\")\n",
        "\n",
        "if data:\n",
        "    with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=list(data[0].keys()))\n",
        "        writer.writeheader()\n",
        "        writer.writerows(data)\n",
        "\n",
        "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"ðŸ’¾ Saved CSV â†’ {csv_path}\")\n",
        "print(f\"ðŸ’¾ Saved JSON â†’ {json_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Qz7lZ1uUFpt",
        "outputId": "91ab88fe-9d74-456d-beba-605569683676"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”¥ Starting scraperâ€¦ going to homepage...\n",
            "\n",
            "ðŸ“„ Scraping page 1 ...\n",
            "   âž¤ Scraped 20 books so farâ€¦ still going strong\n",
            "âœ… Completed page 1\n",
            "\n",
            "ðŸ“„ Scraping page 2 ...\n",
            "   âž¤ Scraped 40 books so farâ€¦ still going strong\n",
            "âœ… Completed page 2\n",
            "\n",
            "ðŸ“„ Scraping page 3 ...\n",
            "   âž¤ Scraped 60 books so farâ€¦ still going strong\n",
            "âœ… Completed page 3\n",
            "\n",
            "ðŸ“„ Scraping page 4 ...\n",
            "   âž¤ Scraped 80 books so farâ€¦ still going strong\n",
            "âœ… Completed page 4\n",
            "\n",
            "ðŸ“„ Scraping page 5 ...\n",
            "   âž¤ Scraped 100 books so farâ€¦ still going strong\n",
            "âœ… Completed page 5\n",
            "\n",
            "ðŸ“„ Scraping page 6 ...\n",
            "   âž¤ Scraped 120 books so farâ€¦ still going strong\n",
            "âœ… Completed page 6\n",
            "\n",
            "ðŸ“„ Scraping page 7 ...\n",
            "   âž¤ Scraped 140 books so farâ€¦ still going strong\n",
            "âœ… Completed page 7\n",
            "\n",
            "ðŸ“„ Scraping page 8 ...\n",
            "   âž¤ Scraped 160 books so farâ€¦ still going strong\n",
            "âœ… Completed page 8\n",
            "\n",
            "ðŸ“„ Scraping page 9 ...\n",
            "   âž¤ Scraped 180 books so farâ€¦ still going strong\n",
            "âœ… Completed page 9\n",
            "\n",
            "ðŸ“„ Scraping page 10 ...\n",
            "   âž¤ Scraped 200 books so farâ€¦ still going strong\n",
            "âœ… Completed page 10\n",
            "\n",
            "ðŸ“„ Scraping page 11 ...\n",
            "   âž¤ Scraped 220 books so farâ€¦ still going strong\n",
            "âœ… Completed page 11\n",
            "\n",
            "ðŸ“„ Scraping page 12 ...\n",
            "   âž¤ Scraped 240 books so farâ€¦ still going strong\n",
            "âœ… Completed page 12\n",
            "\n",
            "ðŸ“„ Scraping page 13 ...\n",
            "   âž¤ Scraped 260 books so farâ€¦ still going strong\n",
            "âœ… Completed page 13\n",
            "\n",
            "ðŸ“„ Scraping page 14 ...\n",
            "   âž¤ Scraped 280 books so farâ€¦ still going strong\n",
            "âœ… Completed page 14\n",
            "\n",
            "ðŸ“„ Scraping page 15 ...\n",
            "   âž¤ Scraped 300 books so farâ€¦ still going strong\n",
            "âœ… Completed page 15\n",
            "\n",
            "ðŸ“„ Scraping page 16 ...\n",
            "   âž¤ Scraped 320 books so farâ€¦ still going strong\n",
            "âœ… Completed page 16\n",
            "\n",
            "ðŸ“„ Scraping page 17 ...\n",
            "   âž¤ Scraped 340 books so farâ€¦ still going strong\n",
            "âœ… Completed page 17\n",
            "\n",
            "ðŸ“„ Scraping page 18 ...\n",
            "   âž¤ Scraped 360 books so farâ€¦ still going strong\n",
            "âœ… Completed page 18\n",
            "\n",
            "ðŸ“„ Scraping page 19 ...\n",
            "   âž¤ Scraped 380 books so farâ€¦ still going strong\n",
            "âœ… Completed page 19\n",
            "\n",
            "ðŸ“„ Scraping page 20 ...\n",
            "   âž¤ Scraped 400 books so farâ€¦ still going strong\n",
            "âœ… Completed page 20\n",
            "\n",
            "ðŸ“„ Scraping page 21 ...\n",
            "   âž¤ Scraped 420 books so farâ€¦ still going strong\n",
            "âœ… Completed page 21\n",
            "\n",
            "ðŸ“„ Scraping page 22 ...\n",
            "   âž¤ Scraped 440 books so farâ€¦ still going strong\n",
            "âœ… Completed page 22\n",
            "\n",
            "ðŸ“„ Scraping page 23 ...\n",
            "   âž¤ Scraped 460 books so farâ€¦ still going strong\n",
            "âœ… Completed page 23\n",
            "\n",
            "ðŸ“„ Scraping page 24 ...\n",
            "   âž¤ Scraped 480 books so farâ€¦ still going strong\n",
            "âœ… Completed page 24\n",
            "\n",
            "ðŸ“„ Scraping page 25 ...\n",
            "   âž¤ Scraped 500 books so farâ€¦ still going strong\n",
            "âœ… Completed page 25\n",
            "\n",
            "ðŸ“„ Scraping page 26 ...\n",
            "   âž¤ Scraped 520 books so farâ€¦ still going strong\n",
            "âœ… Completed page 26\n",
            "\n",
            "ðŸ“„ Scraping page 27 ...\n",
            "   âž¤ Scraped 540 books so farâ€¦ still going strong\n",
            "âœ… Completed page 27\n",
            "\n",
            "ðŸ“„ Scraping page 28 ...\n",
            "   âž¤ Scraped 560 books so farâ€¦ still going strong\n",
            "âœ… Completed page 28\n",
            "\n",
            "ðŸ“„ Scraping page 29 ...\n",
            "   âž¤ Scraped 580 books so farâ€¦ still going strong\n",
            "âœ… Completed page 29\n",
            "\n",
            "ðŸ“„ Scraping page 30 ...\n",
            "   âž¤ Scraped 600 books so farâ€¦ still going strong\n",
            "âœ… Completed page 30\n",
            "\n",
            "ðŸ“„ Scraping page 31 ...\n",
            "   âž¤ Scraped 620 books so farâ€¦ still going strong\n",
            "âœ… Completed page 31\n",
            "\n",
            "ðŸ“„ Scraping page 32 ...\n",
            "   âž¤ Scraped 640 books so farâ€¦ still going strong\n",
            "âœ… Completed page 32\n",
            "\n",
            "ðŸ“„ Scraping page 33 ...\n",
            "   âž¤ Scraped 660 books so farâ€¦ still going strong\n",
            "âœ… Completed page 33\n",
            "\n",
            "ðŸ“„ Scraping page 34 ...\n",
            "   âž¤ Scraped 680 books so farâ€¦ still going strong\n",
            "âœ… Completed page 34\n",
            "\n",
            "ðŸ“„ Scraping page 35 ...\n",
            "   âž¤ Scraped 700 books so farâ€¦ still going strong\n",
            "âœ… Completed page 35\n",
            "\n",
            "ðŸ“„ Scraping page 36 ...\n",
            "   âž¤ Scraped 720 books so farâ€¦ still going strong\n",
            "âœ… Completed page 36\n",
            "\n",
            "ðŸ“„ Scraping page 37 ...\n",
            "   âž¤ Scraped 740 books so farâ€¦ still going strong\n",
            "âœ… Completed page 37\n",
            "\n",
            "ðŸ“„ Scraping page 38 ...\n",
            "   âž¤ Scraped 760 books so farâ€¦ still going strong\n",
            "âœ… Completed page 38\n",
            "\n",
            "ðŸ“„ Scraping page 39 ...\n",
            "   âž¤ Scraped 780 books so farâ€¦ still going strong\n",
            "âœ… Completed page 39\n",
            "\n",
            "ðŸ“„ Scraping page 40 ...\n",
            "   âž¤ Scraped 800 books so farâ€¦ still going strong\n",
            "âœ… Completed page 40\n",
            "\n",
            "ðŸ“„ Scraping page 41 ...\n",
            "   âž¤ Scraped 820 books so farâ€¦ still going strong\n",
            "âœ… Completed page 41\n",
            "\n",
            "ðŸ“„ Scraping page 42 ...\n",
            "   âž¤ Scraped 840 books so farâ€¦ still going strong\n",
            "âœ… Completed page 42\n",
            "\n",
            "ðŸ“„ Scraping page 43 ...\n",
            "   âž¤ Scraped 860 books so farâ€¦ still going strong\n",
            "âœ… Completed page 43\n",
            "\n",
            "ðŸ“„ Scraping page 44 ...\n",
            "   âž¤ Scraped 880 books so farâ€¦ still going strong\n",
            "âœ… Completed page 44\n",
            "\n",
            "ðŸ“„ Scraping page 45 ...\n",
            "   âž¤ Scraped 900 books so farâ€¦ still going strong\n",
            "âœ… Completed page 45\n",
            "\n",
            "ðŸ“„ Scraping page 46 ...\n",
            "   âž¤ Scraped 920 books so farâ€¦ still going strong\n",
            "âœ… Completed page 46\n",
            "\n",
            "ðŸ“„ Scraping page 47 ...\n",
            "   âž¤ Scraped 940 books so farâ€¦ still going strong\n",
            "âœ… Completed page 47\n",
            "\n",
            "ðŸ“„ Scraping page 48 ...\n",
            "   âž¤ Scraped 960 books so farâ€¦ still going strong\n",
            "âœ… Completed page 48\n",
            "\n",
            "ðŸ“„ Scraping page 49 ...\n",
            "   âž¤ Scraped 980 books so farâ€¦ still going strong\n",
            "âœ… Completed page 49\n",
            "\n",
            "ðŸ“„ Scraping page 50 ...\n",
            "   âž¤ Scraped 1000 books so farâ€¦ still going strong\n",
            "âœ… Completed page 50\n",
            "\n",
            "\n",
            "ðŸŽ‰ All done! Total books scraped = 1000\n",
            "\n",
            "ðŸ“¦ Final: collected 1000 books!\n",
            "\n",
            "ðŸ’¾ Saved CSV â†’ output/books_all.csv\n",
            "ðŸ’¾ Saved JSON â†’ output/books_all.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ymUA8m8sULg8"
      },
      "execution_count": 2,
      "outputs": []
    }
  ]
}